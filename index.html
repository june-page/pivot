<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=1200">
    <!-- For social media thumbnail preview -->
    <meta property="og:image" content="https://github.com/june-page/pivot/blob/main/assets/figures/simpsons.jpg?raw=true" />
    <title>RL makes MLLMs see better than SFT</title>
    <!-- Bootstrap CSS -->
    <!-- <link href="vendor/css/bootstrap.min.css" rel="stylesheet"> -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <!-- <link href="vendor/css/all.min.css" rel="stylesheet"> -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts - Nunito as Avenir Next alternative -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@400;700&display=swap" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="styles.css" rel="stylesheet">
    <link href="theme_neutral_light.css" rel="stylesheet">
    <!-- Add this line after your existing stylesheets -->
<!-- <link href="mobile-styles.css" rel="stylesheet"> -->
<link href="nav-pill-fix.css" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->

<!-- @@@@@@@@@@@@@@@@@ HEAD @@@@@@@@@@@@@@@@@ -->
    <header class="header-section" id="overview">
        <div class="container">
            <div class="row">
                <div class="col-lg-11 mx-auto text-center" >
                    <h1 class="paper-title">RL makes MLLMs see better than SFT</h1>
                    <div class="author-line">
                        <span><a href="https://www.junha.page/p/about.html" class="author-link">Junha Song<sup>1,2</sup></a></span>
                        <span><a href="https://sangdooyun.github.io/" class="author-link">Sangdoo Yun<sup>2</sup></a></span>
                        <span><a href="https://sites.google.com/site/dyhan0920/" class="author-link">Dongyoon Han<sup>2</sup></a></span>
                        <span><a href="https://sites.google.com/site/jaegulchoo/" class="author-link">Jaegul Choo<sup>1</sup></a></span>
                        <span><a href="https://sites.google.com/view/byeongho-heo/home" class="author-link">Byeongho Heo<sup>2</sup></a></span>
                    </div>
                    <!-- <div class="equal-contribution mt-2">
                        <span><sup>†</sup>Project lead</span>
                        <span><sup>†</sup>Corresponding author</span>
                    </div> -->
                    <div class="institution-logos mt-4">
                        <div class="institution-logo-wrapper">
                            <img src="assets/logos/kaist.png" alt="KAIST" class="institution-logo">
                            <sup>1</sup>
                        </div>
                        <div class="institution-logo-wrapper">
                            <img src="assets/logos/naver.png" alt="NAVER AI Lav" class="institution-logo">
                            <sup>2</sup>
                        </div>
                    </div>
                    <!-- <div class="mt-4">
                    <div class="paper-subtitle">ICCV 2025 (Highlight)</div>
                    </div> -->
                    <div class="mt-4">
                        <a href="https://www.arxiv.org/abs/2510.16333/" class="btn primary-btn me-2"><i class="fas fa-file-alt me-2"></i>Paper</a>
                        <a href="https://june-page.github.io/pivot/" class="btn secondary-btn me-2"><i class="fab fa-github me-2"></i>Code (coming soon)</a>
                    </div>
                </div>
            </div>
        </div>
    </header>


    <!-- Main Content -->
    <main class="container">
        <!-- New TLDR Section -->

<!-- @@@@@@@@@@@@@@@@@ TLDR @@@@@@@@@@@@@@@@@ -->
<section class="row" id="tldr">
    <div class="col-lg-11 mx-auto">
        <div class="tldr-box p-4 rounded mb-5 mt-4" style="background-color: rgba(112, 237, 119, 0.05); box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);">
            <h2 class="mb-3 text-center" style="color: var(--primary-color); font-size: 2.5rem; font-weight: 700; text-transform: uppercase; letter-spacing: 2px;">TL;DR</h2>
                <div class="row mb-2">
                    <div class="col-md-12">
                        <div class="p-4 mb-2 rounded" style="background-color: rgba(255, 255, 255, 0.9); border: 2px solid rgba(37, 235, 86, 0.3); box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);">
                            <p class="lead fw-bold mb-0" style="color: var(--primary-color); font-size: 1.4rem; text-align: left;">
                                Does the MLLM training method, like SFT versus RL, actually have an impact on the vision encoder's representations?
                                <!-- Does MLLM training reshape the vision encoder’s representation? Which is more effective: DPO or SFT? -->
                            </p>
                        </div>
                    </div>
                </div>
                
                <div class="row mb-4">
                    <div class="col-lg-12">
                        
                        <p class="mb-4 lead" style="font-size: 1.2rem; font-weight: 500; text-align: left;"><strong>Yes—and the training recipe matters!</strong> Our research shows that training with RL (e.g., DPO) produces stronger, more precisely localized visual representations than SFT. This translates to superior performance not only on MLLM benchmarks (especially strongly vision-related VQA tasks) but also on classic vision tasks like ImageNet classification and segmentation. If your goal is to improve the vision encoder itself for MLLM development, RL is the more effective path.</p>
                        <div class="figure-container mb-0 mx-auto" style="box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1); max-width: 800px;">
                            <img src="assets/figures/tldr.png" alt="rl-makes-mllms-see-better" class="img-fluid rounded">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

        


<!-- @@@@@@@@@@@@@@@@@ Motivation @@@@@@@@@@@@@@@@@ -->
<section class="row" id="approach">
    <div class="col-lg-11 mx-auto">
        <h2 class="section-title">Research Motivation</h2>
        <!-- RL vs. SFT in MLLMs & <i>Vision Encoder</i> -->
        
        <div class="row mb-5">
            <div class="col-12 mb-4">
                <div class="card h-100 border-0 shadow-sm motivation-card">
                    <div class="card-body p-4">
                        <div class="motivation-content">
                            <div class="motivation-figure mb-4 mb-md-0">
                                <div class="figure-container gradient-figure">
                                    <img src="assets/figures/hover-pivot-1.jpeg" alt="SFT vs RL illustration" class="img-fluid rounded">
                                </div>
                            </div>
                            <div class="motivation-text">
                                <ul class="motivation-list mb-0">
                                    <li>The dominant 
                                        <span class="study-highlight">
                                        MLLM
                                        <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                            <img src="assets/figures/mllm.png" alt="architecture of mllm">
                                        </span>
                                    </span> 
                                        research paradigm has focused primarily on the LLM backbone or the MLLM itself, leaving the vision encoder under-analyzed.</li>
                                    <li>This oversight impedes a deeper understanding of how modern MLLM training strategies, such as Supervised Finetuning (SFT) and Reinforcement Learning (RL), impact the model.</li>
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </div>
</section>



<!-- @@@@@@@@@@@@@@@@@ MLLM on SFT and RL @@@@@@@@@@@@@@@@@ -->
<section id="mllm-scaling-experiments">
    <div class="row">
        <div class="col-lg-11 mx-auto">
            <h2 class="section-title">How do SFT and RL affect MLLMs?</h2>
            <p>
                The impact of MLLM training strategies is first investigated using 
                <span class="study-highlight">
                    common VQA benchmarks.
                    <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                        <img src="assets/figures/benchmarks.jpg" alt="SFT vs RL comparison graphic">
                    </span>
                </span>
                This contrasts with
                <span class="study-highlight">
                    previous SFT versus RL studies,
                    <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                        <img src="assets/figures/mllm-works.jpg" alt="SFT vs RL comparison graphic">
                    </span>
                </span>
                which performed comparisons in specialized environments like card games or robot action planning.
                Our analysis aims to answer: How do SFT and DPO affect MLLM on diverse VQA tasks? Is DPO actually superior to SFT? Does this trend hold with model scaling?
            </p>

            
            <div class="row mb-5">
                <div class="col-lg-12">
                    <div class="scaling-card mllm-scaling p-4">
                        <div class="row">
                            <div class="col-12 mb-4">
                                <div class="figure-container">
                                    <img src="assets/figures/scaling_mllm_vit.jpeg" alt="vit scaling visualization" class="img-fluid" style="width: 100%; max-width: 1200px; margin: 0 auto; display: block;">
                                    <div class="figure-caption text-center mt-2">
                                        <strong>Scaling the vision encoder in MLLMs:</strong> Performance is reported as the SigLIP2 vision model scales from 86M to 1B with a fixed Qwen2.5-3B language model.
                                    </div>
                                </div>
                            </div>
                            <div class="col-12 mb-4">
                                <div class="figure-container">
                                    <img src="assets/figures/scaling_mllm_lm.jpeg" alt="Model scaling visualization" class="img-fluid" style="width: 100%; max-width: 1200px; margin: 0 auto; display: block;">
                                    <div class="figure-caption text-center mt-2">
                                        <strong>Scaling the language model in MLLMs:</strong> Performance is reported as the Qwen2.5 language model scales from 0.5B to 7B with a fixed SigLIP2-So/16 vision encoder.
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-12">
                                <div class="px-3">
                                    <h4 class="mt-4 mb-3" style="color: var(--secondary-color);">Findings:</h4>
                                    <ul class="findings-list">
                                        <li class="finding-item">The performance improves with the size of the vision encoder, underscoring the importance of the visual representation capacity within MLLMs even though the LM size is also a critical factor.</li>
                                        <li class="finding-item">DPO achieves superior performance compared to SFT, particularly on strongly vision-related tasks, motivating an in-depth analysis of how these learning strategies impact the visual representation in MLLMs.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
        </div>
    </div>
</section>


<!-- @@@@@@@@@@@@@@@@@ Vision on SFT and RL @@@@@@@@@@@@@@@@@ -->
 <script src="question-toggle.js"></script>
<section id="vision-experiments">
    <div class="row">
        <div class="col-lg-11 mx-auto mb-5">
            <h2 class="section-title">How does MLLM training affect visual representations?</h2>
            <p>
                Now, the focus shifts to an in-depth analysis of the vision encoder within MLLMs. As illustrated in
                <span class="study-highlight">
                    our experimental setup
                    <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                        <img src="assets/figures/vision-eval.png" alt="SFT vs RL comparison graphic">
                    </span>
                </span>
                , this is achieved by isolating the encoder and evaluating its standalone performance on classic vision tasks.
            </p>

            <div class="custom-nav-pills mb-4">
                <a class="nav-link" href="index.html#imagenet">ImageNet classification</a>
                <a class="nav-link" href="index.html#gradcam">Gradient visualization</a>
                <a class="nav-link" href="index.html#segmentation">Semantic segmentation</a>
                <a class="nav-link" href="index.html#platonic">Vision & Language alignment</a>
            </div>

            <div class="scaling-card vision-anlaysis p-4">
                <div class="row">

                    <!-- 1. ImageNet -->
                    <div class="col-12 mb-0">
                        <div id="imagenet" class="research-question">
                            <div class="question-header">
                                <div class="question-number">1</div>
                                <h3 class="question-title">Does MLLM training really reshape visual representations?</h3>
                                <button class="question-toggle" type="button" aria-expanded="false">
                                <i class="fas fa-chevron-right" aria-hidden="true"></i>
                                </button>
                            </div>
                            <div class="question-content" id="imagenet-content" hidden>
                                <div class="row align-items-center">
                                    <div class="col-md-6">
                                        <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                            <img src="assets/figures/imagenet.png" alt="ImageNet performance" class="img-fluid rounded">
                                            <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">After MLLM training with either SFT or DPO, the vision encoder is detached and its standalone performance evaluated via linear probing.</p>
                                        </div>
                                    </div>
                                    <div class="col-md-6">
                                        <p><strong>Yes, it does.</strong> In particular, DPO creates better visual representations for ImageNet classification than SFT.
                                        This novel finding shows that the prevalent DPO method (common in the LLM community) is also effective for enhancing visual representations.</p>
                                        <p>Interestingly, MLLM training with larger LLMs yields a high-performing vision encoder, suggesting they provide a more informative optimization signal.</p>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- 2. GradCam -->
                    <div class="col-12 mb-0">
                        <div id="gradcam" class="research-question">
                            <div class="question-header">
                                <div class="question-number">2</div>
                                <h3 class="question-title">Do DPO and SFT provide distinct gradient signals to the vision encoder?</h3>
                                <button class="question-toggle" type="button" aria-expanded="false">
                                <i class="fas fa-chevron-right" aria-hidden="true"></i>
                                </button>
                            </div>
                            <div class="question-content" id="imagenet-content" hidden>
                                <div class="row align-items-center">
                                    <div class="col-md-7">
                                        <p><strong>Yes, and DPO provides a more fine-grained signal. </strong></p>
                                        <p>
                                            <span class="study-highlight">
                                                Our experiment 
                                                <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                                    <img src="assets/figures/gradcam.jpeg" alt="gradcam setup">
                                                </span>
                                            </span>
                                            visualizes gradients from different MLLM training strategies on the vision encoder using <a href="https://arxiv.org/abs/1610.02391" target="_blank" rel="noopener noreferrer">Grad-Cam</a>. 
                                            The results indicate that DPO yields more focused gradients on question-relevant visual features, while the SFT signal is more dispersed. 
                                        </p>
                                        <p>
                                            More results can be found
                                            <span class="study-highlight">
                                                here
                                                <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                                    <img src="assets/figures/gradcam-add.jpeg" alt="gradcam visual 2">
                                                </span>
                                            </span>
                                            .
                                        </p> 
                                    </div>
                                    <div class="col-md-5">
                                        <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                            <img src="assets/figures/gradcam-main.jpeg" alt="gradient visual" class="img-fluid rounded" >
                                            <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">Using Grad-CAM, the gradient signals corresponding to the vision encoder features during MLLM training are visualized.</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- 3. Segmentation -->
                    <div class="col-12 mb-0">
                        <div id="segmentation" class="research-question">
                            <div class="question-header">
                                <div class="question-number">3</div>
                                <h3 class="question-title">Do these distinct gradient signals translate to differences in the vision encoder's localization capabilities?</h3>
                                <button class="question-toggle" type="button" aria-expanded="false">
                                <i class="fas fa-chevron-right" aria-hidden="true"></i>
                                </button>
                            </div>
                            <div class="question-content" id="imagenet-content" hidden>
                                <div class="row align-items-center">
                                    <div class="col-md-6 mt-1">
                                        <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                            <img src="assets/figures/segmen-quan.jpeg" alt="segmentation quantitative" class="img-fluid rounded">
                                            <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">Segmentation performance is evaluated via MLP probing on vision encoders trained with a Qwen2.5-1.5B LLM head.</p>
                                        </div>
                                    </div>
                                    <div class="col-md-6">
                                        <p>Vision encoders trained with DPO and SFT show distinct localization behaviors. Notably, DPO-tuned encoders produce segmentation maps that align more closely with the ground truth.</p>
                                        <p>
                                            <span class="study-highlight">
                                                Here
                                                <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                                    <img src="assets/figures/segmen-qual-add.jpeg" alt="segmentation qualitative-2">
                                                </span>
                                            </span>
                                            are more qualitative results.</p>
                                    </div>
                                    <div class="col-md-12 mt-3">
                                        <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                            <img src="assets/figures/segmen-qual.jpeg" alt="segmentation qualitative" class="img-fluid rounded">
                                            <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">Results from segmentation probing on the CLIP-L/14 336px encoder, which was trained with SFT and DPO within MLLMs, are visualized.</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <!-- 4. Platonic -->
                    <div class="col-12 mb-0">
                        <div id="platonic" class="research-question">
                            <div class="question-header">
                                <div class="question-number">4</div>
                                <h3 class="question-title">What is the impact of MLLM training on vision & language alignment?</h3>
                                <button class="question-toggle" type="button" aria-expanded="false">
                                <i class="fas fa-chevron-right" aria-hidden="true"></i>
                                </button>
                            </div>
                            <div class="question-content" id="imagenet-content" hidden>
                                <div class="row align-items-center">
                                    <div class="col-md-7">
                                        <p>We compute vision–language alignment motivated by the Platonic Representational Hypothesis. The results show that DPO-trained vision encoders achieve stronger alignment scores compared to those trained with SFT.</p>
                                        <p>Moreover, pairing with a larger LLM consistently leads to higher alignment scores, supporting our hypothesis that larger LLMs provide more useful training signals to the vision encoder.</p>
                                    </div>
                                    <div class="col-md-5">
                                        <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                            <img src="assets/figures/platonic.jpeg" alt="platonic" class="img-fluid rounded">
                                            <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">Platonic representational alignment is measured between reference LLMs and the vision encoders of MLLMs trained with Qwen2.5 models (Y-axis: size of the Qwen2.5 model).</p>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="col-md-12">
                        <div class="px-3">
                            <h4 class="mt-4 mb-3" style="color: var(--secondary-color);">Findings:</h4>
                            <ul class="findings-list">
                                <li class="finding-item">MLLM training not only adapts the language model but also reshapes the visual representations.</li>
                                <li class="finding-item">DPO steers the vision encoder toward a more fine-grained analysis of visual information, improving its object localization capabilities.</li>
                                <li class="finding-item">The vision encoder benefits from a larger LLM, which provides more informative backward signals for visual representation within an MLLM.</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<!-- @@@@@@@@@@@@@@@@@ MLLM on SFT and RL @@@@@@@@@@@@@@@@@ -->
<section id="pivot">
    <div class="row">
        <div class="col-lg-11 mx-auto">
            <h2 class="section-title">What’s next: Unlocking vision model potential via RL</h2>
            <div class="motivation-content mb-5">
                <div class="motivation-figure mb-4 mb-md-0">
                    <div class="figure-container gradient-figure">
                        <img src="assets/figures/hover-pivot-2.jpeg" alt="SFT vs RL illustration" class="img-fluid rounded">
                    </div>
                </div>
                <div class="motivation-text">
                    <!-- <ul class="motivation-list mb-0">
                        <li>The dominant 
                            <span class="study-highlight">
                            MLLM
                            <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                <img src="assets/figures/mllm.png" alt="architecture of mllm">
                            </span>
                        </span> 
                            research paradigm has focused primarily on the LLM backbone or the MLLM itself, leaving the vision encoder under-analyzed.</li>
                        <li>This oversight impedes a deeper understanding of how modern MLLM training strategies, such as Supervised Finetuning (SFT) and Reinforcement Learning (RL), impact the model.</li>
                    </ul>ㅋ -->
                    <p>
                        Building on our finding that DPO benefits visual representation learning in MLLMs, we reframe this process as a simple recipe for evolving vision models for MLLMs—Preference-Instructed Vision Optimization (<span class="pivot-font">PIVOT</span>). 
                        Here, we assess the effectiveness of <span class="pivot-font">PIVOT</span>-enhanced representations within MLLMs, 
                        following prior 
                        <span class="study-highlight">
                            evaluation protocols
                            <i class="fa-solid fa-arrow-pointer fa-xs"></i><span class="study-hover-image">
                                <img src="assets/figures/pivot-eval.png" alt="pivot evaluation">
                            </span>
                        </span>
                        such as 
                        <a href="https://cambrian-mllm.github.io/" target="_blank" rel="noopener noreferrer">Cambrian</a> and 
                        <a href="https://davidfan.io/webssl/" target="_blank" rel="noopener noreferrer">Web-SSL</a>.
                        It should be noted that <span class="pivot-font">PIVOT</span> is not proposed as a new method, but rather as an underexplored training regime that enables the development of better MLLMs than those using the original vision encoders.
                    </p>
                </div>
            </div>
            

            
            <div class="row mb-5">
                <div class="col-lg-12">
                    <div class="scaling-card mllm-scaling p-4">
                        <div class="row">
                            <div class="col-12 mb-0">
                                <div id="imagenet" class="pivot-box">
                                    <div class="pivot-box-content" id="imagenet-content">
                                        <div class="row align-items-center">
                                            <div class="col-md-5">
                                                <div class="p-4">
                                                    <img src="assets/figures/pivot-1.jpeg" alt="pivot-1" class="img-fluid rounded">
                                                </div>
                                            </div>
                                            <div class="col-md-7">
                                                <p>
                                                    The results reveal a remarkable impact of <span class="pivot-font">PIVOT</span> when the enhanced encoders are used within MLLMs; 
                                                    a vision model trained with <span class="pivot-font">PIVOT</span> not only outperforms its original counterpart but also surpasses a substantially larger model (e.g., SigLIP2-So/16+<span class="pivot-font">PIVOT</span> > SigLIP2-g/16) 
                                                    and even a subsequent-generation encoder (e.g., SigLIP1-So/14+<span class="pivot-font">PIVOT</span> > SigLIP2-So/16). 
                                                </p>
                                                <p>
                                                    Notably, this enhancement is achieved with just 18 hours of training on 8 H100 GPUs. This amounts to fewer than <strong>1%</strong> of GPUs of standard vision pre-training, with 
                                                    <a href="https://arxiv.org/abs/2502.14786/" target="_blank" rel="noopener noreferrer">SigLIP2</a>
                                                    trained on up to 2K TPUv5e chips.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="col-12 mb-0">
                                <div id="imagenet" class="pivot-box">
                                    <div class="pivot-box-content" id="imagenet-content">
                                        <div class="row align-items-center">
                                            <div class="col-md-5">
                                                <p>When comparing vision encoders trained with different strategies within MLLMs, we find that a vision encoder enhanced with <span class="pivot-font">PIVOT</span> provides a greater advantage over one trained with SFT in MLLM applications.</p>
                                            </div>
                                            <div class="col-md-7">
                                                <div class="p-2">
                                                    <img src="assets/figures/pivot-2.jpeg" alt="pivot-1" class="img-fluid rounded">
                                                </div>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="col-12 mb-0">
                                <div id="imagenet" class="pivot-box">
                                    <div class="pivot-box-content" id="imagenet-content">
                                        <div class="row align-items-center">
                                            <div class="col-md-7">
                                                <div class="p-3">
                                                    <img src="assets/figures/pivot-3.jpeg" alt="pivot-1" class="img-fluid rounded">
                                                    <!-- <p class="text-center mt-1 mb-0" style="font-size: 0.9rem; font-style: italic;">After MLLM training with either SFT or DPO, the vision encoder is detached and its standalone performance evaluated via linear probing.</p> -->
                                                </div>
                                            </div>
                                            <div class="col-md-5">
                                                <p>
                                                    PIVOT also benefits a diverse set of vision encoders, including 
                                                    <a href="https://arxiv.org/abs/2103.00020" target="_blank" rel="noopener noreferrer">CLIP</a>, 
                                                    <a href="https://arxiv.org/abs/2111.06377" target="_blank" rel="noopener noreferrer">MAE</a>, 
                                                    <a href="https://arxiv.org/abs/2304.07193" target="_blank" rel="noopener noreferrer">DINOv2</a>, and ImageNet-SupViT. 
                                                    It is an interesting finding that this benefit extends beyond vision-only self-supervised models such as MAE and 
                                                    <a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener noreferrer">MOCO</a>,
                                                    to vision–language supervised models like CLIP.
                                                </p>
                                            </div>
                                        </div>
                                    </div>
                                </div>
                            </div>
                            <div class="col-md-12">
                                <div class="px-3">
                                    <h4 class="mt-4 mb-3" style="color: var(--secondary-color);">Finding:</h4>
                                    <ul class="findings-list">
                                        <li class="finding-item">Existing vision models possess substantial potential for improvement within MLLMs, which can be unlocked by <span class="pivot-font">PIVOT</span>.</li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
        </div>
    </div>
</section>


<!-- @@@@@@@@@@@@@@@@@ Citation @@@@@@@@@@@@@@@@@ -->
<section id="citation" class="mt-2">
  <div class="row">
    <div class="col-lg-10 mx-auto">
      <h2 class="section-title">Citation</h2>
<div class="citation-box p-5" style="border-radius: 30px;">
<pre style="margin-bottom:0;"><code>@article{song2025rlseebetter,
  title   = {RL makes MLLMs see better than SFT},
  author  = {Junha Song and Sangdoo Yun and Dongyoon Han and Jaegul Choo and Byeongho Heo},
  journal = {arXiv preprint arXiv:2510.16333},
  year    = {2025}
}</code></pre>
</div>
    </div>
  </div>
</section>



<!-- @@@@@@@@@@@@@@@@@ Correspondence @@@@@@@@@@@@@@@@@ -->
<section>
    <div class="row">
        <div class="col-lg-11 mx-auto text-center">
            <h2 class="section-title">Correspondence</h2>
            <p>Please reach out to Junha Song with any questions.</p>
            <p>sb020518@kaist.ac.kr</p>
        </div>
    </div>
</section>

<section>
    <div class="row">
        <div class="col-lg-11 mx-auto text-center">
            <h2 class="section-title">Acknowledgements</h2>
            <p>Special thanks to the 
                <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab" target="_blank" rel="noopener noreferrer"><em>NAVER AI teams</em></a>
                for their generous support.
            </p>  
            <p>
                This project page was developed with reference to the awesome project
                <a href="https://davidfan.io/webssl/" target="_blank" rel="noopener noreferrer"><em>Web-SSL</em></a>.
                We sincerely appreciate the creators for their inspiring work.
              </p>
        </div>
    </div>
</section>







<style>
    .question-header {
        position: relative;
        display: flex;
        align-items: center;
        transition: all 0.3s ease;
    }
    
    .question-header:hover {
        background-color: rgba(37, 235, 86, 0.05);
    }
    
    .question-header .fas {
        transition: transform 0.3s ease;
    }
    
    .question-header[aria-expanded="false"] .fas {
        transform: rotate(-90deg);
    }
    
    .collapse:not(.show) {
        display: none;
    }
    
    .collapse.show {
        display: block;
    }
    
    /* Style for the collapse/expand indicator */
    .ms-auto {
        margin-left: auto !important;
        font-size: 1.2rem;
        padding: 0 10px;
        color: var(--primary-color);
    }
    
    /* Add a subtle visual indicator of collapsiblility */
    .question-header::after {
        content: "";
        position: absolute;
        bottom: 0;
        left: 0;
        right: 0;
        height: 1px;
        background: rgba(0,0,0,0.05);
    }
    </style>
    
    <!-- JavaScript for Question Navigation and Toggle -->
    


    <!-- Bootstrap JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js" type="694a1905905191b476322b11-text/javascript"></script>
    <script type="694a1905905191b476322b11-text/javascript">
        // Select all navigation links in your custom nav pills container
        const navLinks = document.querySelectorAll('.custom-nav-pills a.nav-link');
        
        navLinks.forEach(link => {
            link.addEventListener('click', function (e) {
            // Optionally prevent default if you don't want the page jump
            // e.preventDefault();
        
            // Remove the active class from all links
            navLinks.forEach(btn => btn.classList.remove('active'));
        
            // Add the active class to the clicked link
            this.classList.add('active');
            });
        });

        document.addEventListener('DOMContentLoaded', function() {
        // Handle question navigation
        const navLinks = document.querySelectorAll('.custom-nav-pills a.nav-link');
        
        navLinks.forEach(link => {
            link.addEventListener('click', function(e) {
                // Remove the active class from all links
                navLinks.forEach(btn => btn.classList.remove('active'));
                
                // Add the active class to the clicked link
                this.classList.add('active');
                
                // Optionally expand the target question if it's collapsed
                const targetId = this.getAttribute('href');
                const targetCollapse = document.querySelector(targetId + ' .collapse');
                if (targetCollapse && !targetCollapse.classList.contains('show')) {
                    const headerButton = document.querySelector(targetId + ' .question-header');
                    headerButton.click();
                }
            });
        });
        
        // Update chevron icon when collapse state changes
        const collapseElements = document.querySelectorAll('.collapse');
        
        collapseElements.forEach(collapseEl => {
            collapseEl.addEventListener('shown.bs.collapse', function() {
                const headerIcon = this.previousElementSibling.querySelector('.fas');
                headerIcon.style.transform = 'rotate(0deg)';
            });
            
            collapseEl.addEventListener('hidden.bs.collapse', function() {
                const headerIcon = this.previousElementSibling.querySelector('.fas');
                headerIcon.style.transform = 'rotate(-90deg)';
            });
        });
    });
    </script>
<script src="../cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="694a1905905191b476322b11-|49" defer></script></body>
</html>