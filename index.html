<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- For social media thumbnail preview -->
    <meta property="og:image" content="https://davidfan.io/webssl/assets/figures-old/scale_model_wide_gradient.png" />
    <title>Web-SSL: Scaling Language-Free Visual Representation Learning</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/css/bootstrap.min.css" rel="stylesheet">
    <!-- Font Awesome for icons -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
    <!-- Google Fonts - Nunito as Avenir Next alternative -->
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@300;400;600;700&display=swap" rel="stylesheet">
    <!-- Custom CSS -->
    <link href="styles.css" rel="stylesheet">
    <link href="theme_neutral_light.css" rel="stylesheet">
    <!-- Add this line after your existing stylesheets -->
<link href="mobile-styles.css" rel="stylesheet">
<link href="nav-pill-fix.css" rel="stylesheet">
</head>
<body>
    <!-- Navigation -->

    <!-- Header Section -->
    <header class="header-section" id="overview">
        <div class="container">
            <div class="row">
                <div class="col-lg-11 mx-auto text-center">
                    <h1 class="paper-title">Scaling Language-Free Visual Representation Learning</h1>
                    <div class="author-line">
                        <span><a href="https://davidfan.io/" class="author-link">David Fan<sup>1,*</sup></a></span>
                        <span><a href="https://tsb0601.github.io/" class="author-link">Shengbang Tong<sup>1,2,*</sup></a></span>
                        <span><a href="https://jiachenzhu.github.io/" class="author-link">Jiachen Zhu<sup>1,2</sup></a></span>
                        <span><a href="https://koustuvsinha.com/" class="author-link">Koustuv Sinha<sup>1</sup></a></span>
                        <span><a href="https://liuzhuang13.github.io/" class="author-link">Zhuang Liu<sup>1,3</sup></a></span>
                        <span><a href="https://xinleic.xyz/" class="author-link">Xinlei Chen<sup>1</sup></a></span>
                    </div>
                    <div class="author-line">
                        <span><a href="https://scholar.google.ca/citations?user=cMPKe9UAAAAJ&hl=en" class="author-link">Michael Rabbat<sup>1</sup></a></span>
                        <span><a href="https://scholar.google.com/citations?user=euUV4iUAAAAJ&hl=en" class="author-link">Nicolas Ballas<sup>1</sup></a></span>
                        <span><a href="https://scholar.google.com/citations?user=WLN3QrAAAAAJ&hl=en" class="author-link">Yann LeCun<sup>1,2</sup></a></span>
                        <span><a href="https://www.amirbar.net/" class="author-link">Amir Bar<sup>1,†</sup></a></span>
                        <span><a href="https://www.sainingxie.com/" class="author-link">Saining Xie<sup>2,†</sup></a></span>
                    </div>
                    <div class="equal-contribution mt-2">
                        <span><sup>*</sup> Equal contribution</span>
                        <span class="mx-2">|</span>
                        <span><sup>†</sup> Equal advising</span>
                    </div>
                    <div class="institution-logos mt-4">
                        <div class="institution-logo-wrapper">
                            <img src="assets/logos/meta.png" alt="FAIR, Meta" class="institution-logo">
                            <sup>1</sup>
                        </div>
                        <div class="institution-logo-wrapper">
                            <img src="assets/logos/nyu.png" alt="New York University" class="institution-logo">
                            <sup>2</sup>
                        </div>
                        <div class="institution-logo-wrapper princeton-logo">
                            <img src="assets/logos/princeton.png" alt="Princeton University" class="institution-logo">
                            <sup>3</sup>
                        </div>
                    </div>
                    <div class="mt-4">
                    <div class="paper-subtitle">ICCV 2025 (Highlight)</div>
                    </div>
                    <div class="mt-4">
                        <a href="https://arxiv.org/abs/2504.01017" class="btn primary-btn me-2"><i class="fas fa-file-alt me-2"></i>Paper</a>
                        <a href="https://github.com/facebookresearch/webssl" class="btn secondary-btn me-2"><i class="fab fa-github me-2"></i>Code</a>
                        <a href="https://huggingface.co/collections/facebook/web-ssl-68094132c15fbd7808d1e9bb" class="btn accent-btn"><i class="fas fa-weight-hanging me-2"></i>Model Weights</a>
                    </div>
                </div>
            </div>
        </div>
    </header>



    <!-- News Updates Section - Add right after the header section -->
    <section class="news-updates" id="news">
        <div class="container">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <div class="updates-box p-4 rounded mb-5" style="background-color: rgba(37, 99, 235, 0.05); border-left: 4px solid var(--primary-color); box-shadow: 0 3px 10px rgba(0, 0, 0, 0.05);">
                        <h2 class="section-title mb-3">News & Updates</h2>
                        <ul class="updates-list" style="list-style-type: none; padding-left: 0;">
                            <li class="mb-2 d-flex align-items-start">
                                <span class="update-date me-3" style="min-width: 120px; font-weight: 600; color: var(--primary-color);">June 25, 2025</span>
                                <span class="update-content">WebSSL was accepted to ICCV 2025 as a highlight paper! See you in Hawaii :)</span>
                            </li>
                            <li class="mb-2 d-flex align-items-start">
                                <span class="update-date me-3" style="min-width: 120px; font-weight: 600; color: var(--primary-color);">April 23, 2025</span>
                                <span class="update-content">We are open-sourcing all models from this work at the <a href="https://github.com/facebookresearch/webssl" class="fw-bold">facebookresearch/webssl</a> GitHub repository.</span>
                            </li>
                            <li class="mb-2 d-flex align-items-start">
                                <span class="update-date me-3" style="min-width: 120px; font-weight: 600; color: var(--primary-color);">April 1, 2025</span>
                                <span class="update-content">Our paper "Scaling Language-Free Visual Representation Learning" is now available on arXiv.</span>
                            </li>
                            <!-- You can add more updates here as needed -->
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <style>
        .news-updates {
            margin-top: 10px;
        }
        
        .news-updates .section-title {
            margin-top: 0;
            padding-top: 0;
        }
        
        .updates-list li {
            padding: 8px 0;
            border-bottom: 1px dashed rgba(0,0,0,0.1);
        }
        
        .updates-list li:last-child {
            border-bottom: none;
        }
        
        @media (max-width: 768px) {
            .update-date, .update-content {
                display: block;
            }
            
            .update-date {
                margin-bottom: 5px;
            }
        }
    </style>

    <!-- Main Content -->
    <main class="container">
        <!-- New TLDR Section -->
        <!-- Enhanced TLDR Section -->
<!-- Enhanced TLDR Section -->
<section class="row" id="tldr">
    <div class="col-lg-11 mx-auto">
        <div class="tldr-box p-4 rounded mb-5 mt-4" style="background-color: rgba(191, 203, 229, 0.1); box-shadow: 0 10px 30px rgba(0, 0, 0, 0.08);">
            <h2 class="mb-3 text-center" style="color: var(--primary-color); font-size: 2.5rem; font-weight: 700; text-transform: uppercase; letter-spacing: 2px;">TL;DR</h2>
            <div class="row mb-2">
                <div class="col-md-12">
                    <div class="p-4 mb-2 rounded" style="background-color: rgba(255, 255, 255, 0.9); border: 2px solid rgba(37, 99, 235, 0.3); box-shadow: 0 5px 15px rgba(0, 0, 0, 0.05);">
                        <p class="lead fw-bold mb-0" style="color: var(--primary-color); font-size: 1.5rem; text-align: left;">
                            Is language supervision required to learn effective visual representations for multimodal tasks?
                        </p>
                    </div>
                </div>
            </div>
            
            <div class="row mb-4">
                <div class="col-lg-12">
                    <p class="mb-4 lead" style="font-size: 1.3rem; font-weight: 500; text-align: left;"><strong style="font-size: 1.5rem; color: var(--primary-color);">Not necessarily</strong>! Our research shows that when trained on sufficient web-scale data (2B+ images) and scaled to larger model sizes (7B parameters), visual self-supervised learning models can match or even outperform language-supervised models like CLIP across a broad range of visual question answering tasks — including OCR and chart understanding — without using any language supervision.</p>
                    
                    <div class="figure-container mb-0 mx-auto" style="box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1); max-width: 800px;">
                        <img src="assets/figures-old/fig1_simple_v2.png" alt="Web-SSL Scaling Performance" class="img-fluid rounded">
                    </div>
                </div>
            </div>
            
            <div class="key-findings mt-4">
                <div class="row">
                    <div class="col-md-4 mb-3">
                        <div class="finding-card p-4 rounded shadow-sm" style="border-top: 5px solid var(--primary-color); height: 100%; transform: translateY(0); transition: all 0.3s ease;">
                            <h5 style="color: var(--primary-color); font-size: 1.25rem;"><i class="fas fa-chart-line me-2"></i>Model Scaling Works</h5>
                            <p class="mb-0" style="font-size: 1.1rem;">Web-DINO improves consistently with model size (+4.6% from 1B to 7B params), while CLIP plateaus beyond 3B parameters.</p>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3">
                        <div class="finding-card p-4 rounded shadow-sm" style="border-top: 5px solid var(--secondary-color); height: 100%; transform: translateY(0); transition: all 0.3s ease;">
                            <h5 style="color: var(--secondary-color); font-size: 1.25rem;"><i class="fas fa-database me-2"></i>Data Scale Matters</h5>
                            <p class="mb-0" style="font-size: 1.1rem;">OCR & Chart understanding improves dramatically (+12.6%) as training data increases from 1B to 8B examples.</p>
                        </div>
                    </div>
                    <div class="col-md-4 mb-3">
                        <div class="finding-card p-4 rounded shadow-sm" style="border-top: 5px solid var(--accent-color); height: 100%; transform: translateY(0); transition: all 0.3s ease;">
                            <h5 style="color: var(--accent-color); font-size: 1.25rem;"><i class="fas fa-language me-2"></i>No Language Needed</h5>
                            <p class="mb-0" style="font-size: 1.1rem;">Web-DINO at 7B parameters outperforms CLIP on multimodal tasks without any language supervision during training.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

        

    
        <!-- Visual SSL 2.0 -->
        <section class="row" id="approach">
            <div class="col-lg-11 mx-auto">
                <h2 class="section-title">Web-SSL: Visual SSL 1.0 &#8594 <i>2.0</i></h2>
                
                <div class="row mb-5">
                    <!-- Box 1: Training Data -->
                    <div class="col-md-4 mb-4">
                        <div class="card h-100 border-0 shadow-sm">
                            <div class="card-body p-4 text-center">
                                <div style="width: 70px; height: 70px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; margin: 0 auto 20px auto;">
                                    <i class="fas fa-database fa-2x text-white"></i>
                                </div>
                                <h4 class="card-title">Web-Scale Data</h4>
                                <p class="card-text">Using the MetaCLIP dataset (2B images) to control for data distribution differences and elevate visual SSL to a new data regime</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Box 2: Model Scale -->
                    <div class="col-md-4 mb-4">
                        <div class="card h-100 border-0 shadow-sm">
                            <div class="card-body p-4 text-center">
                                <div style="width: 70px; height: 70px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; margin: 0 auto 20px auto;">
                                    <i class="fas fa-microchip fa-2x text-white"></i>
                                </div>
                                <h4 class="card-title">Parameter Scaling</h4>
                                <p class="card-text">Training Vision Transformers from 1B to 7B parameters to find the ceiling of visual SSL and identify emergent capabilities</p>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Box 3: Evaluation -->
                    <div class="col-md-4 mb-4">
                        <div class="card h-100 border-0 shadow-sm">
                            <div class="card-body p-4 text-center">
                                <div style="width: 70px; height: 70px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; margin: 0 auto 20px auto;">
                                    <i class="fas fa-tasks fa-2x text-white"></i>
                                </div>
                                <h4 class="card-title">Comprehensive Evaluation</h4>
                                <p class="card-text">Rigorous assessment across 16 diverse VQA benchmarks that span General, Knowledge, OCR & Chart, and Vision-Centric categories</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Rest of the content remains unchanged -->

        <!-- Vision Model Scales Section -->
        <section id="scaling-experiments">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <h2 class="section-title">Scaling Visual SSL</h2>
                    
                    <!-- Model Scaling Visualization -->
                    <div class="row mb-5">
                        <div class="col-lg-12">
                            <div class="scaling-card model-scaling p-4">
                                <h3 class="scaling-title text-center mb-4">Effect of Model Scaling</h3>
                                <div class="row">
                                    <!-- Chart visualization -->
                                    <div class="col-12 mb-4">
                                        <div class="figure-container">
                                            <img src="assets/figures-old/scale_model_wide_gradient.png" alt="Model scaling visualization showing Web-DINO outperforming CLIP at larger sizes" class="img-fluid" style="width: 100%; max-width: 1200px; margin: 0 auto; display: block;">
                                            <div class="figure-caption text-center mt-3">
                                                <strong>Model scaling:</strong> Web-DINO performance improves consistently with increasing parameter count (+4.9% from 1B to 7B), while CLIP performance plateaus beyond 3B parameters (+0.7% from 1B to 7B). All models are trained on 2B images from MetaCLIP (MC-2B).
                                            </div>
                                        </div>
                                    </div>
                                    <!-- Key insights -->
                                    <div class="col-md-5 mt-2">
                                        <div class="p-3 rounded" style="background-color: rgba(37, 99, 235, 0.05);">
                                            <h5 class="mb-3">Average VQA Performance of Web-DINO Trained on MC-2B</h5>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">1B params</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 60%; background-color: #93c5fd;" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">49.0%</span>
                                            </div>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">3B params</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 75%; background-color: #60a5fa;" aria-valuenow="75" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">51.7%</span>
                                            </div>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">5B params</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 85%; background-color: #3b82f6;" aria-valuenow="85" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">52.8%</span>
                                            </div>
                                            <div class="d-flex align-items-center">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">7B params</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 95%; background-color: #2563eb;" aria-valuenow="95" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">53.9%</span>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="col-md-7 mt-2">
                                        <div class="findings-box">
                                            <h5 class="mb-3">Key Findings</h5>
                                            <ul class="mb-0 ps-3">
                                                <li class="mb-2"><strong>Scaling Behavior</strong>: Web-DINO improves consistently with model size.</li>
                                                <li class="mb-2"><strong>Task-Specific Gains</strong>: OCR & Chart performance improves the most (+8.2%), followed by Vision-Centric (+5.9%) from 1B to 7B params.</li>
                                                <li class="mb-2"><strong>DINO vs. CLIP</strong>: Web-DINO matches CLIP at 5B parameters, and outperforms CLIP at 7B parameters. </li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <!-- Data Scaling Visualization -->
                    <div class="row mt-2">
                        <div class="col-lg-12">
                            <div class="scaling-card data-scaling p-4">
                                <h3 class="scaling-title text-center mb-4">Effect of Number of Training Samples</h3>
                                <div class="row">
                                    <!-- Chart visualization -->
                                    <div class="col-12 mb-4">
                                        <div class="figure-container">
                                            <img src="assets/figures-old/scaling_dino_data_with_clip.png" alt="Data scaling visualization showing continuous improvement with more training data" class="img-fluid" style="width: 100%; max-width: 1200px; margin: 0 auto; display: block;">
                                            <div class="figure-caption text-center mt-3">
                                                <strong>Training data size analysis:</strong> Web-DINO ViT-7B continues to improve with more training samples (+4.2% from 1B to 8B), with OCR & Chart tasks exhibiting the most significant improvements (+12.6% from 1B to 8B).
                                            </div>
                                        </div>
                                    </div>
                                    <!-- Key insights -->
                                    <div class="col-md-5 mt-2">
                                        <div class="p-3 rounded" style="background-color: rgba(255, 158, 27, 0.05);">
                                            <h5 class="mb-3">OCR & Chart Performance of Web-DINO</h5>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">1B examples</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 60%; background-color: #fdba74;" aria-valuenow="60" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">26.8%</span>
                                            </div>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">2B examples</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 70%; background-color: #fb923c;" aria-valuenow="70" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">31.3%</span>
                                            </div>
                                            <div class="d-flex align-items-center mb-2">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">4B examples</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 85%; background-color: #f97316;" aria-valuenow="85" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">35.6%</span>
                                            </div>
                                            <div class="d-flex align-items-center">
                                                <span style="width: 80px; text-align: left; font-size: 0.9rem;">8B examples</span>
                                                <div class="progress flex-grow-1" style="height: 10px;">
                                                    <div class="progress-bar" role="progressbar" style="width: 95%; background-color: #ea580c;" aria-valuenow="95" aria-valuemin="0" aria-valuemax="100"></div>
                                                </div>
                                                <span class="ms-2" style="width: 50px; text-align: right; font-size: 0.9rem;">39.3%</span>
                                            </div>
                                        </div>
                                    </div>
                                    <div class="col-md-7 mt-2">
                                        <div class="findings-box">
                                            <h5 class="mb-3">Task-Specific Insights</h5>
                                            <ul class="mb-0 ps-3">
                                                <li class="mb-2"><strong>General VQA</strong>: Slight improvement (+1.1%) with diminishing returns after 2B examples.</li>
                                                <li class="mb-2"><strong>Knowledge VQA</strong>: Modest improvements (+1.5%) with diminishing returns after 2B examples.</li>
                                                <li class="mb-2"><strong>OCR & Chart</strong>: Significant improvements with no sign of saturation (+12.6%).</li>
                                                <li><strong>DINO vs. CLIP</strong>: Web-DINO consistently outperforms CLIP per fixed amount of training data.</li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Questions & Analysis Section with Collapsible Functionality -->
<section id="research-questions" class="mt-2">
    <div class="row">
        <div class="col-lg-11 mx-auto">
            <h2 class="section-title">Analysis of Scaling Results</h2>
            
            <!-- Navigation Pills for Questions -->
            <div class="custom-nav-pills mb-5">
                <a class="nav-link active" href="index.html#question1">Question 1: Generality to other Methods</a>
                <a class="nav-link" href="index.html#question2">Question 2: Smaller Dataset Size</a>
                <a class="nav-link" href="index.html#question3">Question 3: Classic Vision Benchmarks</a>
                <a class="nav-link" href="index.html#question4">Question 4: OCR Performance</a>
                <a class="nav-link" href="index.html#question5">Question 5: Language Alignment</a>
            </div>

            
            <div id="question1" class="research-question">
                <div class="question-header" data-bs-toggle="collapse" data-bs-target="#collapseQuestion1" aria-expanded="true" aria-controls="collapseQuestion1" style="cursor: pointer;">
                    <div class="question-number">1</div>
                    <h3 class="question-title">Does the observed scaling behavior generalize to other visual SSL methods?</h3>
                    <div class="ms-auto">
                        <i class="fas fa-chevron-down"></i>
                    </div>
                </div>
                <div id="collapseQuestion1" class="collapse show question-answer">
                    <p class="lead fw-bold" style="color: var(--primary-color); border-bottom: 1px solid rgba(0,0,0,0.1); padding-bottom: 10px; margin-bottom: 15px;">
                        Yes, both joint embedding methods (Web-DINO) and masked modeling methods (Web-MAE) exhibit similar scaling properties, though with distinctive performance characteristics.
                    </p>
                    <div class="row align-items-center">
                        <div class="col-md-6">
                            <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                <img src="assets/figures-old/scaling_with_mae.png" alt="Web-MAE vs Web-DINO performance comparison" class="img-fluid rounded">
                                <p class="text-center mt-2 mb-0" style="font-size: 0.9rem; font-style: italic;">Comparison of Web-DINO and Web-MAE performance across model scales (1B-5B parameters)</p>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="px-3">
                                <h4 class="mb-3" style="color: var(--secondary-color);">Findings:</h4>
                                <ul class="findings-list">
                                    <li class="finding-item">Web-MAE and Web-DINO improve consistently with model size (+2.3% for Web-MAE and +3.9% for Web-DINO from 1B to 5B params on average VQA)</li>
                                    <li class="finding-item">Web-MAE achieves better OCR & Chart performance (+2.5% compared to Web-DINO at 5B parameters)</li>
                                </ul>
                                <p>These results demonstrate that the observed scaling behavior generalizes across different visual SSL methods, suggesting that this phenomenon is intrinsic to the visual self-supervised paradigm, rather than any specific method.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id="question2" class="research-question">
                <div class="question-header" data-bs-toggle="collapse" data-bs-target="#collapseQuestion2" aria-expanded="false" aria-controls="collapseQuestion2" style="cursor: pointer;">
                    <div class="question-number">2</div>
                    <h3 class="question-title">Does visual SSL exhibit similar scaling behavior on smaller, conventional datasets?</h3>
                    <div class="ms-auto">
                        <i class="fas fa-chevron-down"></i>
                    </div>
                </div>
                <div id="collapseQuestion2" class="collapse question-answer">
                    <p class="lead fw-bold" style="color: var(--primary-color); border-bottom: 1px solid rgba(0,0,0,0.1); padding-bottom: 10px; margin-bottom: 15px;">
                        No, models trained on smaller datasets such as ImageNet-1k (1.2M unique images) improve negligibly with model size, highlighting the critical importance of diverse web-scale data.
                    </p>
                    <div class="row align-items-center">
                        <div class="col-md-6">
                            <div class="px-3">
                                <div class="comparison-chart p-3 rounded" style="background-color: rgba(90, 115, 2, 0.05); border-left: 4px solid var(--secondary-color);">
                                    <h4 class="mb-3" style="color: var(--secondary-color);">Data Distribution Comparison</h4>
                                    <div class="d-flex align-items-center mb-3">
                                        <div style="width: 120px; height: 20px; background: linear-gradient(90deg, #e9ecef 30%, #ced4da 100%); border-radius: 10px;" class="me-3"></div>
                                        <span>ImageNet-1k (1.2M unique images): <strong>Negligible improvement</strong> (-0.1% from 1B to 3B)</span>
                                    </div>
                                    <div class="d-flex align-items-center">
                                        <div style="width: 120px; height: 20px; background: linear-gradient(90deg, #2563EB 30%, #1d4ed8 100%); border-radius: 10px;" class="me-3"></div>
                                        <span>MetaCLIP data (2B+ unique images): <strong>Noticeable improvement</strong> (+2.7% from 1B to 3B)</span>
                                    </div>
                                </div>
                                <div class="mt-4">
                                    <p>This parallels observations from language model research where data diversity and scale are essential for effective model scaling.</p>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                <img src="assets/figures-old/pretrain_data_comparison_avg_first.png" alt="Comparison of ImageNet vs MC-2B training" class="img-fluid rounded">
                                <p class="text-center mt-2 mb-0" style="font-size: 0.9rem; font-style: italic;">Performance comparison between models trained on ImageNet-1k vs. MetaCLIP data across model sizes</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id="question3" class="research-question">
                <div class="question-header" data-bs-toggle="collapse" data-bs-target="#collapseQuestion3" aria-expanded="false" aria-controls="collapseQuestion3" style="cursor: pointer;">
                    <div class="question-number">3</div>
                    <h3 class="question-title">How do scaled models perform on classic vision tasks?</h3>
                    <div class="ms-auto">
                        <i class="fas fa-chevron-down"></i>
                    </div>
                </div>
                <div id="collapseQuestion3" class="collapse question-answer">
                    <p class="lead fw-bold" style="color: var(--primary-color); border-bottom: 1px solid rgba(0,0,0,0.1); padding-bottom: 10px; margin-bottom: 15px;">
                        Web-DINO models maintain strong performance on traditional vision benchmarks while improving on VQA tasks. But there are no clear scaling trends on classic vision tasks with increased model size.
                    </p>
                    <div class="row align-items-center">
                        <div class="col-md-6">
                            <div class="px-3">
                                <div class="task-performance mb-4">
                                    <h4 class="mb-3" style="color: var(--secondary-color);">Performance on Classic Vision Tasks</h4>
                                    <table class="table table-sm" style="text-align: center">
                                        <thead>
                                            <tr>
                                                <th>Model</th>
                                                <th>ImageNet-1k</th>
                                                <th>ADE20K</th>
                                                <th>NYU Depth &#8595</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <td>MetaCLIP ViT-G</td>
                                                <td>86.4%</td>
                                                <td>46.7%</td>
                                                <td>0.415</td>
                                            </tr>
                                            <tr>
                                                <td>DINOv2 ViT-g</td>
                                                <td>86.5%</td>
                                                <td>53.0%</td>
                                                <td>0.298</td>
                                            </tr>
                                            <tr>
                                                <td>Web-DINO ViT-1B</td>
                                                <td>84.7%</td>
                                                <td>51.0%</td>
                                                <td>0.345</td>
                                            </tr>
                                            <tr>
                                                <td>Web-DINO ViT-7B</td>
                                                <td>86.0%</td>
                                                <td>54.7%</td>
                                                <td>0.339</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </div>
                                
                                <p>Unlike VQA, classic vision performance improves modestly with increased parameter count.</p>
                                
                                <p>This result highlights VQA's value as a complementary evaluation protocol that may better reflect real-world perceptual challenges.</p>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                <img src="assets/figures-old/vision_eval_perf.png" alt="Classic vision task performance" class="img-fluid rounded">
                                <p class="text-center mt-2 mb-0" style="font-size: 0.9rem; font-style: italic;">Performance trends on ImageNet, ADE20K and NYU Depth.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div id="question4" class="research-question">
                <div class="question-header" data-bs-toggle="collapse" data-bs-target="#collapseQuestion4" aria-expanded="false" aria-controls="collapseQuestion4" style="cursor: pointer;">
                    <div class="question-number">4</div>
                    <h3 class="question-title">Why does scaling visual SSL improve OCR & Chart performance?</h3>
                    <div class="ms-auto">
                        <i class="fas fa-chevron-down"></i>
                    </div>
                </div>
                <div id="collapseQuestion4" class="collapse question-answer">
                    <p class="lead fw-bold" style="color: var(--primary-color); border-bottom: 1px solid rgba(0,0,0,0.1); padding-bottom: 10px; margin-bottom: 15px;">
                        Web-scale datasets naturally contain text-rich images that enable visual SSL models to learn OCR capabilities without explicit language supervision. Strategic data filtering further enhances this effect.
                    </p>
                    
                    <div class="row mb-4">
                        <div class="col-md-10 mx-auto">
                            <div class="figure-container">
                                <div class="row">
                                    <div class="col-md-4 d-flex flex-column align-items-center">
                                        <div style="width: 225px; height: 225px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                            <img src="assets/figures-old/rawdata.png" alt="Raw data examples" style="width: 300px; height: 225px; object-fit: cover;" class="rounded">
                                        </div>
                                        <p class="text-center" style="font-size: 0.9rem; width: 100%;">Raw MC-2B Data</p>
                                    </div>
                                    <div class="col-md-4 d-flex flex-column align-items-center">
                                        <div style="width: 225px; height: 225px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                            <img src="assets/figures-old/lightfilter.png" alt="Light filtered data examples" style="width: 300px; height: 225px; object-fit: cover;" class="rounded">
                                        </div>
                                        <p class="text-center" style="font-size: 0.9rem; width: 100%;">Light Filter (50.3%)</p>
                                    </div>
                                    <div class="col-md-4 d-flex flex-column align-items-center">
                                        <div style="width: 225px; height: 225px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 10px;">
                                            <img src="assets/figures-old/heavyfilter.png" alt="Heavy filtered data examples" style="width: 300px; height: 225px; object-fit: cover;" class="rounded">
                                        </div>
                                        <p class="text-center" style="font-size: 0.9rem; width: 100%;">Heavy Filter (1.3%)</p>
                                    </div>
                                </div>
                                <div class="figure-caption text-center mt-3">
                                    <strong>Examples of data filtering.</strong> <i>Left:</i> Random samples from MC-2B. <i>Middle</i>: Images with text (50.3% of data). <i>Right</i>: Charts, tables and documents (1.3% of data).
                                </div>
                            </div>
                        </div>
                    </div>
                    
                    <div class="row mt-4">
                        <div class="col-md-11 mx-auto">
                          <h4 class="text-center mb-4">Understanding OCR & Chart Performance Improvements with Data Filtering</h4>
                          
                          <!-- Two-column layout for improved breakdown and insights -->
                          <div class="row mx-0">
                            <!-- Left column: OCR & Chart Task Breakdown -->
                            <div class="col-md-6 mb-4 px-2">
                              <div class="card shadow-sm h-100 border-0">
                                <div class="card-header bg-primary text-white py-2">
                                  <h5 class="mb-0 text-white">OCR & Chart Performance Breakdown</h5>
                                </div>
                                <div class="card-body">
                                  <!-- Task improvement bars with percentages -->
                                  <div class="mb-3">
                                    <div class="d-flex justify-content-between align-items-center mb-1">
                                      <span class="font-weight-bold">ChartQA</span>
                                      <span class="text-success font-weight-bold">+24.2%</span>
                                    </div>
                                    <div class="progress" style="height: 12px;">
                                      <div class="progress-bar bg-success" role="progressbar" style="width: 96.8%;" 
                                        aria-valuenow="24.2" aria-valuemin="0" aria-valuemax="25"></div>
                                    </div>
                                    <div class="d-flex justify-content-between mt-1">
                                      <small>Full Data: 23.3%</small>
                                      <small>Heavy Filter: 47.5%</small>
                                    </div>
                                  </div>
                                  
                                  <div class="mb-3">
                                    <div class="d-flex justify-content-between align-items-center mb-1">
                                      <span class="font-weight-bold">OCRBench</span>
                                      <span class="text-success font-weight-bold">+13.8%</span>
                                    </div>
                                    <div class="progress" style="height: 12px;">
                                      <div class="progress-bar bg-success" role="progressbar" style="width: 55.2%;" 
                                        aria-valuenow="13.8" aria-valuemin="0" aria-valuemax="25"></div>
                                    </div>
                                    <div class="d-flex justify-content-between mt-1">
                                      <small>Full Data: 15.6%</small>
                                      <small>Heavy Filter: 29.4%</small>
                                    </div>
                                  </div>
                                  
                                  <div class="mb-3">
                                    <div class="d-flex justify-content-between align-items-center mb-1">
                                      <span class="font-weight-bold">TextVQA</span>
                                      <span class="text-success font-weight-bold">+3.6%</span>
                                    </div>
                                    <div class="progress" style="height: 12px;">
                                      <div class="progress-bar bg-success" role="progressbar" style="width: 14.4%;" 
                                        aria-valuenow="3.6" aria-valuemin="0" aria-valuemax="25"></div>
                                    </div>
                                    <div class="d-flex justify-content-between mt-1">
                                      <small>Full Data: 49.2%</small>
                                      <small>Heavy Filter: 52.8%</small>
                                    </div>
                                  </div>
                                  
                                  <div class="mb-2">
                                    <div class="d-flex justify-content-between align-items-center mb-1">
                                      <span class="font-weight-bold">DocVQA</span>
                                      <span class="text-success font-weight-bold">+13.0%</span>
                                    </div>
                                    <div class="progress" style="height: 12px;">
                                      <div class="progress-bar bg-success" role="progressbar" style="width: 52%;" 
                                        aria-valuenow="13" aria-valuemin="0" aria-valuemax="25"></div>
                                    </div>
                                    <div class="d-flex justify-content-between mt-1">
                                      <small>Full Data: 19.0%</small>
                                      <small>Heavy Filter: 32.0%</small>
                                    </div>
                                  </div>
                                  
                                  <!-- Overall improvement -->
                                  <div class="alert alert-success mt-3 py-2">
                                    <strong>Overall OCR & Chart Improvement: +13.6%</strong> (from 26.8% to 40.4%)
                                  </div>
                                </div>
                              </div>
                            </div>
                            
                            <!-- Right column: Unified Key Insights -->
                            <div class="col-md-6 mb-4 px-2">
                              <div class="card shadow-sm h-100 border-0">
                                <div class="card-header bg-secondary text-white py-2">
                                  <h5 class="mb-0 text-white">Key Findings & Mechanisms</h5>
                                </div>
                                <div class="card-body">
                                  <!-- Performance Improvements -->
                                  <div class="mb-3 pb-2 border-bottom">
                                    <div class="d-flex">
                                      <div style="min-width: 32px; height: 32px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; color: white; margin-right: 10px;">
                                        <i class="fas fa-chart-line"></i>
                                      </div>
                                      <div>
                                        <h5 class="fs-5">Major Performance Gains</h5>
                                        <ul class="mb-0 ps-3 small">
                                          <li><strong>Light Filter (50.3%):</strong> +6.4% on OCR & Chart tasks</li>
                                          <li><strong>Heavy Filter (1.3%):</strong> +13.6% on OCR & Chart tasks</li>
                                          <li><strong>ChartQA:</strong> +24.2% improvement with heavy filtering</li>
                                        </ul>
                                      </div>
                                    </div>
                                  </div>
                                  
                                  <div class="mb-3 pb-2 border-bottom">
                                    <div class="d-flex">
                                      <div style="min-width: 32px; height: 32px; border-radius: 50%; background-color: var(--accent-color); display: flex; align-items: center; justify-content: center; color: white; margin-right: 10px;">
                                        <i class="fas fa-trophy"></i>
                                      </div>
                                      <div>
                                        <h5 class="fs-5">Surpassing Language-Supervised Models</h5>
                                        <p class="mb-0 small">Heavy-filtered Web-DINO (40.4%) outperforms CLIP (36.1%) on OCR & Chart tasks despite using no language supervision and only 1.3% of training data.</p>
                                      </div>
                                    </div>
                                  </div>

                                  <!-- Mechanistic Explanation -->
                                  <div class="mb-3 pb-2 border-bottom">
                                    <div class="d-flex">
                                      <div style="min-width: 32px; height: 32px; border-radius: 50%; background-color: var(--secondary-color); display: flex; align-items: center; justify-content: center; color: white; margin-right: 10px;">
                                        <i class="fas fa-cogs"></i>
                                      </div>
                                      <div>
                                        <h5 class="fs-5">Why It Works</h5>
                                        <p class="mb-0 small">Visual self-supervised learning effectively extracts textual information from images without language supervision. Web-scale datasets naturally contain text-rich images, and strategic data filtering significantly enhances this capability.</p>
                                      </div>
                                    </div>
                                  </div>
                                  
                                  <!-- Outperforming CLIP -->
                                  
                                  
                                  <!-- Strategic Implications -->
                                  
                                </div>
                              </div>
                            </div>
                          </div>
                          
                          
                        </div>
                      </div>
                
                </div>
            </div>
            
            <div id="question5" class="research-question">
                <div class="question-header" data-bs-toggle="collapse" data-bs-target="#collapseQuestion5" aria-expanded="false" aria-controls="collapseQuestion5" style="cursor: pointer;">
                    <div class="question-number">5</div>
                    <h3 class="question-title">Why can SSL learn strong visual representations for multimodal modeling without language supervision?</h3>
                    <div class="ms-auto">
                        <i class="fas fa-chevron-down"></i>
                    </div>
                </div>
                <div id="collapseQuestion5" class="collapse question-answer">
                    <p class="lead fw-bold" style="color: var(--primary-color); border-bottom: 1px solid rgba(0,0,0,0.1); padding-bottom: 10px; margin-bottom: 15px;">
                        As model size and data increase, SSL models naturally develop representational alignment with language models, despite receiving no language supervision.
                    </p>
                    <div class="row align-items-center">
                        <div class="col-md-6">
                            <div class="px-3">
                                <div class="alignment-factors mt-4">
                                    <h4 class="mb-3" style="color: var(--secondary-color);">Possible Factors Driving Language Alignment</h4>
                                    <div class="d-flex align-items-center">
                                        <div style="width: 60px; height: 40px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; color: white; font-weight: 600;" class="me-3">
                                            <i class="fas fa-database"></i>
                                        </div>
                                        <div><strong>Web-scale training data</strong> exposes models to diverse visual concepts and even images containing text.</div>
                                    </div>
                                    <div class="d-flex align-items-center">
                                        <div style="width: 90px; height: 40px; border-radius: 50%; background-color: var(--primary-color); display: flex; align-items: center; justify-content: center; color: white; font-weight: 600;" class="me-3">
                                            <i class="fas fa-microchip"></i>
                                        </div>
                                        <div><strong>Increased model capacity and more training data</strong> enables learning more comprehensive and abstract representations, that implicitly capture linguistic concepts.</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-6">
                            <div class="p-3 rounded shadow-sm" style="background: linear-gradient(135deg, #f8f9fa, #e9ecef);">
                                <img src="assets/figures-old/alignment_scatter_plot.png" alt="LLM alignment visualization" class="img-fluid rounded">
                                <p class="text-center mt-2 mb-0" style="font-size: 0.9rem; font-style: italic;">Measurement of representational alignment between visual features and Llama-3 8B/70B language models, <i>without</i> any finetuning or alignment procedure.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>



        <!-- Key Takeaways Section -->
        <section id="conclusions" class="mt-2">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <h2 class="section-title">Key Takeaways</h2>
                    
                    <div class="highlight-box p-4" style="background-color: rgba(37, 99, 235, 0.05); border-radius: 15px; border-left: 8px solid var(--primary-color);">
                        <div class="row">
                            <div class="col-md-12">
                                <ul class="findings-list" style="list-style-type: none; padding-left: 0;">
                                    <li class="mb-4">
                                        <div class="d-flex align-items-center">
                                            <span class="badge bg-primary p-2 me-3" style="font-size: 1rem; min-width: 36px; text-align: center;">1</span>
                                            <div>
                                                <h4 class="mb-1">Visual SSL Scales</h4>
                                                <p class="mb-0">Visual SSL models improve consistently with both increasing model size and more training data. In contrast, CLIP models plateau beyond moderate sizes.</p>
                                            </div>
                                        </div>
                                    </li>
                                    
                                    <li class="mb-4">
                                        <div class="d-flex align-items-center">
                                            <span class="badge bg-primary p-2 me-3" style="font-size: 1rem; min-width: 36px; text-align: center;">2</span>
                                            <div>
                                                <h4 class="mb-1">Pretraining Data Distribution Matters A Lot</h4>
                                                <p class="mb-0">SSL models trained on traditional, smaller datasets like ImageNet-1k (1.2M images) show negligible improvements with increased parameter count. Diverse web-scale data is necessary to enable effective scaling of visual SSL models.</p>
                                            </div>
                                        </div>
                                    </li>
                                    
                                    <li class="mb-4">
                                        <div class="d-flex align-items-center">
                                            <span class="badge bg-primary p-2 me-3" style="font-size: 1rem; min-width: 36px; text-align: center;">3</span>
                                            <div>
                                                <h4 class="mb-1">Text-Rich Images Enhance OCR Capabilities</h4>
                                                <p class="mb-0">Training on a data subset containing text-rich images (1.3% of total data) significantly improves OCR & Chart understanding, even outperforming CLIP models of the same size trained on full data.</p>
                                            </div>
                                        </div>
                                    </li>
                                    
                                    <!-- <li>
                                        <div class="d-flex align-items-center">
                                            <span class="badge bg-primary p-2 me-3" style="font-size: 1rem; min-width: 36px; text-align: center;">4</span>
                                            <div>
                                                <h4 class="mb-1">Emergent Language Alignment</h4>
                                                <p class="mb-0">Visual SSL models naturally develop representations that increasingly align with language models as they scale (+12.5% alignment score improvements), suggesting an intrinsic connection between visual and linguistic understanding, that emerges without explicit multimodal alignment.</p>
                                            </div>
                                        </div>
                                    </li> -->
                                </ul>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Resources Section -->
        <section id="resources" class="mt-2">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <h2 class="section-title">Resources</h2>
                    <div class="row g-4">
                        <div class="col-md-4">
                            <div class="resources-card card">
                                <div class="card-body text-center">
                                    <div class="key-point-icon">
                                        <i class="fas fa-file-alt"></i>
                                    </div>
                                    <h3 class="card-title">Paper</h3>
                                    <p class="card-text">Read our paper for detailed methodology, results, and analysis.</p>
                                    <a href="https://arxiv.org/abs/2504.01017" class="btn primary-btn">Download Paper</a>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="resources-card card">
                                <div class="card-body text-center">
                                    <div class="key-point-icon">
                                        <i class="fab fa-github"></i>
                                    </div>
                                    <h3 class="card-title">Code & Models</h3>
                                    <p class="card-text">Access open-source model definitions and inference code for Web-SSL.</p>
                                    <a href="https://github.com/facebookresearch/webssl" class="btn secondary-btn">GitHub Repository</a>
                                </div>
                            </div>
                        </div>
                        <div class="col-md-4">
                            <div class="resources-card card">
                                <div class="card-body text-center">
                                    <div class="key-point-icon">
                                        <i class="fas fa-weight-hanging"></i>
                                    </div>
                                    <h3 class="card-title">Model Weights</h3>
                                    <p class="card-text">Download our Web-SSL model weights for your research.</p>
                                    <a href="https://huggingface.co/collections/facebook/web-ssl-68094132c15fbd7808d1e9bb" class="btn accent-btn">Download Models</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Citation Section -->
        <section id="citation" class="mt-2">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <h2 class="section-title">Citation</h2>
                    <div class="bg-light p-4 rounded">
                        <pre><code>@article{fan2025scaling,
  title={Scaling Language-Free Visual Representation Learning},
  author={Fan, David and Tong, Shengbang and Zhu, Jiachen and Sinha, Koustuv and Liu, Zhuang and Chen, Xinlei and Rabbat, Michael and Ballas, Nicolas and LeCun, Yann and Bar, Amir and Xie, Saining},
  journal={arXiv preprint arXiv:2504.01017},
  year={2025}
}</code></pre>
                    </div>
                </div>
            </div>
        </section>
        <!-- Frequently Asked Questions Section -->
        <section id="faq" class="mt-2">
            <div class="row">
                <div class="col-lg-11 mx-auto">
                    <h2 class="section-title">FAQ</h2>
                    <div class="accordion" id="faqAccordion">
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingOne">
                                <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
                                    What is the difference between SSL and CLIP?
                                </button>
                            </h2>
                            <div id="collapseOne" class="accordion-collapse collapse show" aria-labelledby="headingOne" data-bs-parent="#faqAccordion">
                                <div class="accordion-body">
                                    <p>Visual Self-Supervised Learning (SSL) methods learn representations from images alone, using various pretext tasks like contrastive learning or masked image modeling. In contrast, Contrastive Language-Image Pretraining (CLIP) learns from paired image-text data, creating representations that align visual features with linguistic semantics. The primary distinction is that SSL operates without language supervision, while CLIP explicitly leverages language to guide representation learning.</p>
                                </div>
                            </div>
                        </div>
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingTwo">
                                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
                                    How do you evaluate vision models on VQA tasks?
                                </button>
                            </h2>
                            <div id="collapseTwo" class="accordion-collapse collapse" aria-labelledby="headingTwo" data-bs-parent="#faqAccordion">
                                <div class="accordion-body">
                                    <p>We use a controlled two-stage visual instruction tuning procedure. First, a lightweight MLP adapter projects the vision encoder features into the LLM dimensionality, with only this adapter being trained. In the second stage, both the MLP adapter and LLM are finetuned. Critically, the vision encoder remains frozen in both stages, enabling fair comparison across different vision encoders. All experiments use the same LLM backbone (Llama-3 8B Instruct) and identical training data from Cambrian-Alignment and Cambrian-7M datasets to ensure consistent evaluation.</p>
                                </div>
                            </div>
                        </div>
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingThree">
                                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseThree" aria-expanded="false" aria-controls="collapseThree">
                                    Why do our visual SSL models perform well on OCR & Chart tasks?
                                </button>
                            </h2>
                            <div id="collapseThree" class="accordion-collapse collapse" aria-labelledby="headingThree" data-bs-parent="#faqAccordion">
                                <div class="accordion-body">
                                    <p>Web-scale image datasets naturally contain significant textual information. Unlike object-centric datasets like ImageNet, web images frequently include text in the form of labels, signs, charts, and documents. Our research demonstrates that with sufficient training data (2B+ images) and model capacity (5B+ parameters), visual SSL models can develop effective text recognition capabilities without explicit language supervision. We found that strategic filtering for text-rich images further enhances OCR & Chart performance (+13.6% improvement), allowing SSL models to outperform comparable CLIP models (+4.3%) with only 1.3% of the original unique training images.</p>
                                </div>
                            </div>
                        </div>
                        <div class="accordion-item">
                            <h2 class="accordion-header" id="headingFour">
                                <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseFour" aria-expanded="false" aria-controls="collapseFour">
                                    Will you release the models?
                                </button>
                            </h2>
                            <div id="collapseFour" class="accordion-collapse collapse" aria-labelledby="headingFour" data-bs-parent="#faqAccordion">
                                <div class="accordion-body">
                                    <p>Yes, we are open sourceing our models here: https://github.com/facebookresearch/webssl </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

    <section>
        <div class="row">
            <div class="col-lg-11 mx-auto text-center">
                <h2 class="section-title">Correspondence</h2>
                <p>Please reach out to David Fan and Shengbang (Peter) Tong with any questions.</p>
                <p>davidfan [at] meta [dot] com</p>
                <p>st5087 [at] nyu [dot] edu</p>
            </div>
        </div>
    </section>

</main>

    <!-- Footer -->
    <!-- <footer class="footer">
        <div class="container">
            <div class="row">
                <div class="col-lg-10 mx-auto text-center"> -->
                    <!-- <div class="mb-4">
                        <a href="#" class="text-light mx-2"><i class="fab fa-github fa-lg"></i></a>
                        <a href="#" class="text-light mx-2"><i class="fab fa-twitter fa-lg"></i></a>
                        <a href="#" class="text-light mx-2"><i class="fas fa-globe fa-lg"></i></a>
                    </div> -->
                    <!-- <p>Have questions? Please email David Fan and Shengbang (Peter) Tong.</p>
                </div>
            </div>
        </div>
    </footer> -->

<!-- Additional CSS Styles for Collapsible Questions -->
<style>
    .question-header {
        position: relative;
        display: flex;
        align-items: center;
        transition: all 0.3s ease;
    }
    
    .question-header:hover {
        background-color: rgba(37, 99, 235, 0.05);
    }
    
    .question-header .fas {
        transition: transform 0.3s ease;
    }
    
    .question-header[aria-expanded="false"] .fas {
        transform: rotate(-90deg);
    }
    
    .collapse:not(.show) {
        display: none;
    }
    
    .collapse.show {
        display: block;
    }
    
    /* Style for the collapse/expand indicator */
    .ms-auto {
        margin-left: auto !important;
        font-size: 1.2rem;
        padding: 0 10px;
        color: var(--primary-color);
    }
    
    /* Add a subtle visual indicator of collapsiblility */
    .question-header::after {
        content: "";
        position: absolute;
        bottom: 0;
        left: 0;
        right: 0;
        height: 1px;
        background: rgba(0,0,0,0.05);
    }
    </style>
    
    <!-- JavaScript for Question Navigation and Toggle -->
    


    <!-- Bootstrap JS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.2/js/bootstrap.bundle.min.js" type="694a1905905191b476322b11-text/javascript"></script>
    <script type="694a1905905191b476322b11-text/javascript">
        // Select all navigation links in your custom nav pills container
        const navLinks = document.querySelectorAll('.custom-nav-pills a.nav-link');
        
        navLinks.forEach(link => {
            link.addEventListener('click', function (e) {
            // Optionally prevent default if you don't want the page jump
            // e.preventDefault();
        
            // Remove the active class from all links
            navLinks.forEach(btn => btn.classList.remove('active'));
        
            // Add the active class to the clicked link
            this.classList.add('active');
            });
        });

        document.addEventListener('DOMContentLoaded', function() {
        // Handle question navigation
        const navLinks = document.querySelectorAll('.custom-nav-pills a.nav-link');
        
        navLinks.forEach(link => {
            link.addEventListener('click', function(e) {
                // Remove the active class from all links
                navLinks.forEach(btn => btn.classList.remove('active'));
                
                // Add the active class to the clicked link
                this.classList.add('active');
                
                // Optionally expand the target question if it's collapsed
                const targetId = this.getAttribute('href');
                const targetCollapse = document.querySelector(targetId + ' .collapse');
                if (targetCollapse && !targetCollapse.classList.contains('show')) {
                    const headerButton = document.querySelector(targetId + ' .question-header');
                    headerButton.click();
                }
            });
        });
        
        // Update chevron icon when collapse state changes
        const collapseElements = document.querySelectorAll('.collapse');
        
        collapseElements.forEach(collapseEl => {
            collapseEl.addEventListener('shown.bs.collapse', function() {
                const headerIcon = this.previousElementSibling.querySelector('.fas');
                headerIcon.style.transform = 'rotate(0deg)';
            });
            
            collapseEl.addEventListener('hidden.bs.collapse', function() {
                const headerIcon = this.previousElementSibling.querySelector('.fas');
                headerIcon.style.transform = 'rotate(-90deg)';
            });
        });
    });
    </script>
<script src="../cdn-cgi/scripts/7d0fa10a/cloudflare-static/rocket-loader.min.js" data-cf-settings="694a1905905191b476322b11-|49" defer></script></body>
</html>